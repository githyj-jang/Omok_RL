{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/githyj-jang/Omok_RL/blob/main/Omok_with_A3C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import threading\n",
        "import random\n",
        "import gym\n",
        "import time\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "from collections import deque"
      ],
      "metadata": {
        "id": "a4oQLnZitVGw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81c49827-0580-421b-e71d-063e70ed7e84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 오목 보드 크기\n",
        "BOARD_SIZE = 15\n",
        "\n",
        "GAMMA = 0.99\n",
        "LEARNING_RATE = 0.01\n",
        "GLOBAL_UPDATE_FREQUENCY = 10\n",
        "NUM_WORKERS = 4\n",
        "MAX_EPISODES = 1000"
      ],
      "metadata": {
        "id": "WaNUVgFduBrA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 글로벌 네트워크 정의\n",
        "class GlobalNetwork(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(GlobalNetwork, self).__init__()\n",
        "        self.conv1 = Conv2D(32, (3, 3), activation='relu', input_shape=(BOARD_SIZE, BOARD_SIZE, 1))\n",
        "        self.conv2 = Conv2D(64, (3, 3), activation='relu')\n",
        "        self.flatten = Flatten()\n",
        "        self.dense = Dense(128, activation='relu')\n",
        "        self.policy_logits = Dense(BOARD_SIZE * BOARD_SIZE)\n",
        "        self.value = Dense(1)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.conv1(inputs)\n",
        "        x = self.conv2(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.dense(x)\n",
        "        logits = self.policy_logits(x)\n",
        "        value = self.value(x)\n",
        "        return logits, value"
      ],
      "metadata": {
        "id": "NY6Mf9eNuCaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 오목 게임 환경 정의\n",
        "class OmokEnv:\n",
        "    def __init__(self, board_size=BOARD_SIZE):\n",
        "        self.board_size = board_size\n",
        "        self.board = np.zeros((board_size, board_size))\n",
        "        self.current_player = 1  # 1은 흰돌, -1은 검은돌\n",
        "\n",
        "    def reset(self):\n",
        "        self.board = np.zeros((self.board_size, self.board_size))\n",
        "        self.current_player = 1\n",
        "        return self.board\n",
        "\n",
        "    def step(self, action):\n",
        "        available_actions = get_available_actions(self.board)\n",
        "        if not available_actions:\n",
        "            return self.board, 0, True, {}\n",
        "        x, y = action\n",
        "        if self.board[x, y] == 0:\n",
        "            self.board[x, y] = self.current_player\n",
        "            reward = get_reward(self.board, self.current_player, action)\n",
        "            done = is_terminal_state(self.board)\n",
        "            self.current_player *= -1\n",
        "            return self.board, reward, done, {}\n",
        "        else:\n",
        "            return self.board, 0, False, {}\n",
        "\n",
        "    def render(self):\n",
        "        for row in self.board:\n",
        "            print(' '.join(['.' if x == 0 else 'O' if x == 1 else 'X' for x in row]))\n",
        "        print()"
      ],
      "metadata": {
        "id": "V9SuKfbSuHs7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 가능한 행동 반환 함수\n",
        "def get_available_actions(board):\n",
        "    return [(i, j) for i in range(BOARD_SIZE) for j in range(BOARD_SIZE) if board[i][j] == 0]\n",
        "\n",
        "# reward 계산 함수\n",
        "def get_reward(board, player, action):\n",
        "    x, y = action\n",
        "    reward = 0\n",
        "    if check_win(board, player):\n",
        "        return 1  # 승리 시 보상\n",
        "    if check_win(board, -player):\n",
        "        return -1  # 패배 시 보상\n",
        "    directions = [(1, 0), (0, 1), (1, 1), (1, -1)]\n",
        "    for dx, dy in directions:\n",
        "        count_player = count_consecutive_stones(board, player, x, y, dx, dy)\n",
        "        count_opponent = count_consecutive_stones(board, -player, x, y, dx, dy)\n",
        "        # 연속된 돌의 수에 따라 보상 부여\n",
        "        if count_player == 4:\n",
        "            reward += 0.05  # 4개의 연속된 돌을 놓는다면 큰 보상\n",
        "        elif count_player == 3:\n",
        "            reward += 0.02  # 3개의 연속된 돌을 놓는다면 보상\n",
        "        elif count_player == 2:\n",
        "            reward += 0.01  # 2개의 연속된 돌을 놓는다면 작은 보상\n",
        "        # 상대방의 연속된 돌을 막는다면 보상\n",
        "        if count_opponent == 4:\n",
        "            reward += 0.05\n",
        "        elif count_opponent == 3:\n",
        "            reward += 0.02\n",
        "        elif count_opponent == 2:\n",
        "            reward += 0.01\n",
        "    return reward\n",
        "\n",
        "# 연속된 돌 개수 계산 함수\n",
        "def count_consecutive_stones(board, player, x, y, dx, dy):\n",
        "    count = 0\n",
        "    while 0 <= x < BOARD_SIZE and 0 <= y < BOARD_SIZE and board[x][y] == player:\n",
        "        count += 1\n",
        "        x += dx\n",
        "        y += dy\n",
        "    x -= dx\n",
        "    y -= dy\n",
        "    while 0 <= x < BOARD_SIZE and 0 <= y < BOARD_SIZE and board[x][y] == player:\n",
        "        count += 1\n",
        "        x -= dx\n",
        "        y -= dy\n",
        "    return count - 1\n",
        "\n",
        "# 승리 조건 체크 함수\n",
        "def check_win(board, player):\n",
        "    def check_direction(x, y, dx, dy):\n",
        "        count = 0\n",
        "        for _ in range(5):\n",
        "            if 0 <= x < BOARD_SIZE and 0 <= y < BOARD_SIZE and board[x][y] == player:\n",
        "                count += 1\n",
        "            else:\n",
        "                break\n",
        "            x += dx\n",
        "            y += dy\n",
        "        return count == 5\n",
        "\n",
        "    for i in range(BOARD_SIZE):\n",
        "        for j in range(BOARD_SIZE):\n",
        "            if (check_direction(i, j, 1, 0) or  # 가로\n",
        "                check_direction(i, j, 0, 1) or  # 세로\n",
        "                check_direction(i, j, 1, 1) or  # 대각선 (\\)\n",
        "                check_direction(i, j, 1, -1)):  # 대각선 (/)\n",
        "                return True\n",
        "    return False\n",
        "\n",
        "# 게임 종료 여부 체크 함수 (승리 또는 무승부)\n",
        "def is_terminal_state(board):\n",
        "    return check_win(board, 1) or check_win(board, -1) or all(all(cell != 0 for cell in row) for row in board)\n",
        "\n",
        "# 행동 선택 함수\n",
        "def choose_action(state, available_actions, policy):\n",
        "    if not available_actions:\n",
        "        return random.choice(get_available_actions(state))\n",
        "    logits, _ = policy(state.reshape(1, BOARD_SIZE, BOARD_SIZE, 1))\n",
        "    prob = tf.nn.softmax(logits)\n",
        "    action_index = np.argmax(prob.numpy())\n",
        "\n",
        "    if action_index >= len(available_actions):\n",
        "      print(\"Warning: action_index is out of bounds for available_actions\")\n",
        "      return None\n",
        "\n",
        "    return available_actions[action_index]\n"
      ],
      "metadata": {
        "id": "n_rDkLW8uLmH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 워커 정의\n",
        "class Worker(threading.Thread):\n",
        "    def __init__(self, global_model, optimizer, worker_id):\n",
        "        super(Worker, self).__init__()\n",
        "        self.global_model = global_model\n",
        "        self.optimizer = optimizer\n",
        "        self.worker_id = worker_id\n",
        "        self.local_model = GlobalNetwork()\n",
        "        self.env = OmokEnv()\n",
        "\n",
        "    def run(self):\n",
        "        total_step = 1\n",
        "        while total_step <= MAX_EPISODES:\n",
        "            current_state = self.env.reset()\n",
        "            done = False\n",
        "            step = 0\n",
        "            while not done:\n",
        "                step += 1\n",
        "                available_actions = get_available_actions(current_state)\n",
        "                action = choose_action(current_state, available_actions, self.local_model)\n",
        "                row, col = action\n",
        "                next_state, reward, done, _ = self.env.step((row, col))\n",
        "\n",
        "                with tf.GradientTape() as tape:\n",
        "                    logits, value = self.local_model(tf.convert_to_tensor(current_state.reshape(1, BOARD_SIZE, BOARD_SIZE, 1), dtype=tf.float32))\n",
        "                    _, next_value = self.local_model(tf.convert_to_tensor(next_state.reshape(1, BOARD_SIZE, BOARD_SIZE, 1), dtype=tf.float32))\n",
        "                    advantage = reward + GAMMA * next_value * (1 - int(done)) - value\n",
        "                    policy_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=[row * BOARD_SIZE + col], logits=logits)\n",
        "                    value_loss = advantage ** 2\n",
        "                    loss = policy_loss + value_loss\n",
        "\n",
        "                grads = tape.gradient(loss, self.local_model.trainable_weights)\n",
        "                self.optimizer.apply_gradients(zip(grads, self.global_model.trainable_weights))\n",
        "                self.local_model.set_weights(self.global_model.get_weights())\n",
        "\n",
        "                current_state = next_state\n",
        "                total_step += 1\n",
        "\n",
        "                if done:\n",
        "                    print(f\"Worker {self.worker_id} Episode {total_step} Reward {reward}\")"
      ],
      "metadata": {
        "id": "hiP7Z66juOvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "# 메인 실행 부분\n",
        "global_model = GlobalNetwork()\n",
        "global_model(tf.convert_to_tensor(np.zeros((1, BOARD_SIZE, BOARD_SIZE, 1)), dtype=tf.float32))  # 모델 빌드\n",
        "optimizer = Adam(learning_rate=LEARNING_RATE)\n",
        "\n",
        "workers = []\n",
        "for i in range(NUM_WORKERS):\n",
        "    worker = Worker(global_model, optimizer, i)\n",
        "    workers.append(worker)\n",
        "\n",
        "for worker in workers:\n",
        "    worker.start()\n",
        "\n",
        "for worker in workers:\n",
        "    worker.join()\n",
        "\n",
        "print(\"A3C 학습 완료\")\n",
        "\n",
        "elapsed_time = time.time() - start_time\n",
        "\n",
        "print(elapsed_time)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8EqAMnFuQ2A",
        "outputId": "4012c736-b4c5-4af6-93e9-bd6da11c212c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Worker 0 Episode 45 Reward 1\n",
            "Worker 2 Episode 63 Reward 1\n",
            "Worker 1 Episode 62 Reward 1\n",
            "Worker 3 Episode 67 Reward 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception in thread Thread-29:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
            "Exception in thread Thread-28:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"<ipython-input-49-fa8bd705760e>\", line 20, in run\n",
            "  File \"<ipython-input-48-ef9762923006>\", line 78, in choose_action\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n",
            "    raise e.with_traceback(filtered_tb) from None\n",
            "  File \"<ipython-input-46-38e62529152d>\", line 15, in call\n",
            "tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling layer 'flatten_23' (type Flatten).\n",
            "\n",
            "{{function_node __wrapped__Reshape_device_/job:localhost/replica:0/task:0/device:GPU:0}} Input to reshape is a tensor with 7744 values, but the requested shape has 1 [Op:Reshape] name: \n",
            "\n",
            "Call arguments received by layer 'flatten_23' (type Flatten):\n",
            "  • inputs=tf.Tensor(shape=(1, 11, 11, 64), dtype=float32)\n",
            "    self.run()\n",
            "  File \"<ipython-input-49-fa8bd705760e>\", line 32, in run\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/backprop.py\", line 1066, in gradient\n",
            "    flat_grad = imperative_grad.imperative_grad(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/imperative_grad.py\", line 67, in imperative_grad\n",
            "    return pywrap_tfe.TFE_Py_TapeGradient(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/backprop.py\", line 148, in _gradient_function\n",
            "    return grad_fn(mock_op, *out_grads)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/array_grad.py\", line 808, in _ReshapeGrad\n",
            "    array_ops.reshape(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/weak_tensor_ops.py\", line 88, in wrapper\n",
            "    return op(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\", line 153, in error_handler\n",
            "    raise e.with_traceback(filtered_tb) from None\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py\", line 5883, in raise_from_not_ok_status\n",
            "    raise core._status_to_exception(e) from None  # pylint: disable=protected-access\n",
            "tensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __wrapped__Reshape_device_/job:localhost/replica:0/task:0/device:GPU:0}} Input to reshape is a tensor with 7744 values, but the requested shape has 1 [Op:Reshape] name: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Worker 0 Episode 146 Reward 1\n",
            "Worker 1 Episode 137 Reward 1\n",
            "Worker 0 Episode 203 Reward 1\n",
            "Worker 1 Episode 194 Reward 1\n",
            "Worker 0 Episode 260 Reward 1\n",
            "Worker 1 Episode 251 Reward 1\n",
            "Worker 0 Episode 317 Reward 1\n",
            "Worker 1 Episode 308 Reward 1\n",
            "Worker 0 Episode 374 Reward 1\n",
            "Worker 1 Episode 365 Reward 1\n",
            "Worker 0 Episode 431 Reward 1\n",
            "Worker 1 Episode 422 Reward 1\n",
            "Worker 0 Episode 488 Reward 1\n",
            "Worker 1 Episode 479 Reward 1\n",
            "Worker 0 Episode 545 Reward 1\n",
            "Worker 1 Episode 536 Reward 1\n",
            "Worker 0 Episode 602 Reward 1\n",
            "Worker 0 Episode 659 Reward 1\n",
            "Worker 1 Episode 593 Reward 1\n",
            "Worker 0 Episode 716 Reward 1\n",
            "Worker 1 Episode 650 Reward 1\n",
            "Worker 0 Episode 773 Reward 1\n",
            "Worker 1 Episode 707 Reward 1\n",
            "Worker 0 Episode 830 Reward 1\n",
            "Worker 1 Episode 764 Reward 1\n",
            "Worker 0 Episode 887 Reward 1\n",
            "Worker 1 Episode 821 Reward 1\n",
            "Worker 0 Episode 944 Reward 1\n",
            "Worker 1 Episode 868 Reward 1\n",
            "Worker 0 Episode 995 Reward 1\n",
            "Worker 0 Episode 1082 Reward 1\n",
            "Worker 1 Episode 970 Reward 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception in thread Thread-27:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"<ipython-input-49-fa8bd705760e>\", line 21, in run\n",
            "TypeError: cannot unpack non-iterable NoneType object\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: action_index is out of bounds for available_actions\n",
            "A3C 학습 완료\n",
            "110.95767951011658\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jG81rxMusw2E",
        "outputId": "59a2ad50-5c0a-47cf-ddd6-c23e83ae7de7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Player 1 (학습된 모델)의 승률: 100.0%\n",
            "Player 2 (무작위 행동)의 승률: 0.0%\n",
            "무승부 비율: 0.0%\n"
          ]
        }
      ],
      "source": [
        "def evaluate_model(global_model, episodes=100):\n",
        "    env = OmokEnv()\n",
        "    player1_wins = 0\n",
        "    player2_wins = 0\n",
        "    draws = 0\n",
        "    for _ in range(episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            # Player 1 (학습된 모델)의 행동 선택\n",
        "            available_actions = get_available_actions(state)\n",
        "            action = choose_action(state, available_actions, global_model)\n",
        "            state, reward, done, _ = env.step(action)\n",
        "\n",
        "            if done:\n",
        "                if reward == 1:\n",
        "                    player1_wins += 1\n",
        "                elif reward == -1:\n",
        "                    player2_wins += 1\n",
        "                else:\n",
        "                    draws += 1\n",
        "\n",
        "            if not done:\n",
        "                # Player 2 (무작위 행동)의 행동 선택\n",
        "                available_actions = get_available_actions(state)\n",
        "                action = random.choice(available_actions)\n",
        "                state, reward, done, _ = env.step(action)\n",
        "\n",
        "                if done:\n",
        "                  if reward == 1:\n",
        "                      player1_wins += 1\n",
        "                  elif reward == -1:\n",
        "                      player2_wins += 1\n",
        "                  else:\n",
        "                      draws += 1\n",
        "\n",
        "    player1_win_rate = player1_wins / episodes\n",
        "    player2_win_rate = player2_wins / episodes\n",
        "    draw_rate = draws / episodes\n",
        "\n",
        "    return player1_win_rate, player2_win_rate, draw_rate\n",
        "\n",
        "player1_win_rate, player2_win_rate, draw_rate = evaluate_model(global_model, episodes=100)\n",
        "\n",
        "print(f\"Player 1 (학습된 모델)의 승률: {player1_win_rate * 100}%\")\n",
        "print(f\"Player 2 (무작위 행동)의 승률: {player2_win_rate * 100}%\")\n",
        "print(f\"무승부 비율: {draw_rate * 100}%\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model_equal(global_model, episodes=100):\n",
        "    env = OmokEnv()\n",
        "    player1_wins = 0\n",
        "    player2_wins = 0\n",
        "    draws = 0\n",
        "    for _ in range(episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            # Player 1 (학습된 모델)의 행동 선택\n",
        "            available_actions = get_available_actions(state)\n",
        "            action = choose_action(state, available_actions, global_model)\n",
        "            state, reward, done, _ = env.step(action)\n",
        "\n",
        "            if done:\n",
        "                if reward == 1:\n",
        "                    player1_wins += 1\n",
        "                elif reward == -1:\n",
        "                    player2_wins += 1\n",
        "                else:\n",
        "                    draws += 1\n",
        "\n",
        "            if not done:\n",
        "                # Player 2 (학습된 모델)의 행동 선택\n",
        "                available_actions = get_available_actions(state)\n",
        "                action = choose_action(state, available_actions, global_model)\n",
        "                state, reward, done, _ = env.step(action)\n",
        "\n",
        "                if done:\n",
        "                  if reward == 1:\n",
        "                      player1_wins += 1\n",
        "                  elif reward == -1:\n",
        "                      player2_wins += 1\n",
        "                  else:\n",
        "                      draws += 1\n",
        "\n",
        "\n",
        "    player1_win_rate = player1_wins / episodes\n",
        "    player2_win_rate = player2_wins / episodes\n",
        "    draw_rate = draws / episodes\n",
        "\n",
        "    return player1_win_rate, player2_win_rate, draw_rate\n",
        "\n",
        "player1_win_rate, player2_win_rate, draw_rate = evaluate_model_equal(global_model, episodes=100)\n",
        "\n",
        "print(f\"Player 1 (학습된 모델)의 승률: {player1_win_rate * 100}%\")\n",
        "print(f\"Player 2 (학습된 모델)의 승률: {player2_win_rate * 100}%\")\n",
        "print(f\"무승부 비율: {draw_rate * 100}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8e0NSX3yPWE",
        "outputId": "8a80f60d-e75c-4a7f-d7f9-4847c87c998b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Player 1 (학습된 모델)의 승률: 100.0%\n",
            "Player 2 (학습된 모델)의 승률: 0.0%\n",
            "무승부 비율: 0.0%\n"
          ]
        }
      ]
    }
  ]
}